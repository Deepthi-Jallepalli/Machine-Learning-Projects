{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CbhuTgwDB4mG"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyQz-XGhkkD9"
   },
   "source": [
    "Given train.dat file conatins inputs of documents and frequency of terms in all the documnets. Read the data as csr matrix where every row is a document and every column is termID and the cell contains the frequency of the termID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7VoFSYBlDkI5"
   },
   "outputs": [],
   "source": [
    "def convert_dat_csrmatrix(filename):\n",
    "    \n",
    "    row_id = []\n",
    "    column_id = []\n",
    "    frequency = []\n",
    "    \n",
    "    file = open(filename,'r')\n",
    "    \n",
    "    for i, eachline in enumerate(file):\n",
    "        eachline = eachline.strip()\n",
    "        data = eachline.split()\n",
    "        \n",
    "        for index in range(len(data)):\n",
    "            \n",
    "            if index % 2 == 0:\n",
    "                column_id.append(int(data[index]))\n",
    "                frequency.append(int(data[index + 1]))\n",
    "                row_id.append(i)\n",
    "    \n",
    "    matrix = csr_matrix((frequency, (row_id, column_id)), dtype = np.float)\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LgpkC3ulkuBK"
   },
   "source": [
    "We have the term frequency , inorder find the the frequency and importance of the terms while clustering find TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF) using L2 norm : The cosine similarity between two vectors is their dot product when l2 norm has been applied on the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oa7G1v_9DoTW"
   },
   "outputs": [],
   "source": [
    "def calculate_tfidf(matrix):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    transformer = TfidfTransformer(norm='l2', use_idf=True,smooth_idf=False)\n",
    "    dataset_tfidf = transformer.fit_transform(matrix)\n",
    "    \n",
    "    return dataset_tfidf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQhqTco0nuBy"
   },
   "source": [
    "Inorder reduce the computation time and To avoid risk of overfitting model result in terrible out of sample performance we need to reduce the dimenstionality of the matrix to n columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPUjVYxEDrsr"
   },
   "outputs": [],
   "source": [
    "def reduce_dimensionality(matrix_tfid):\n",
    "    \n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=150)\n",
    "    reduced = svd.fit_transform(matrix_tfid)\n",
    "    \n",
    "    return reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSRob7Q0opij"
   },
   "source": [
    "Choosing 2 random rows data and assign them as initial centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7dIXVD26DufG"
   },
   "outputs": [],
   "source": [
    "def initialize_centroids(reduced_matrix,K):\n",
    "    \n",
    "    init_centroids = reduced_matrix[np.random.choice(reduced_matrix.shape[0], size=K, replace=False),:]\n",
    "\n",
    "    return init_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOqigSoto281"
   },
   "source": [
    "Using euclidean_distance measure to find the optimal centroids - that is distance between the centroids and given document data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9-UIbFvD3oE"
   },
   "outputs": [],
   "source": [
    "def calculate_euclidean_dist(centroid_points,reduced_matrix_rows):\n",
    "    \n",
    "    return np.sqrt(np.sum(np.power(centroid_points - reduced_matrix_rows,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wy3rsOs1D6sb"
   },
   "outputs": [],
   "source": [
    "def calculate_optimalcentroids(reduced_matrix,initial_centroids,initial_clusters,K,rows):\n",
    "    \n",
    "    optimal_centroids = True\n",
    "    while optimal_centroids:\n",
    "        optimal_centroids = False\n",
    "        for row in range(rows):\n",
    "            d = np.inf\n",
    "            index = -1\n",
    "            for cluster_index in range(K):\n",
    "                euclid_dist = calculate_euclidean_dist(initial_centroids[cluster_index,:],reduced_matrix[row,:])\n",
    "                if euclid_dist < d :\n",
    "                    d = euclid_dist\n",
    "                    index = cluster_index\n",
    "            if initial_clusters[row,0] != index:\n",
    "                optimal_centroids = True\n",
    "            initial_clusters[row,:] = index , d**2\n",
    "            \n",
    "        for i in range(K):\n",
    "            cluster_datapoints = reduced_matrix[np.nonzero(initial_clusters[:,0].A == i)[0]]\n",
    "            initial_centroids[i,:] = np.mean(cluster_datapoints,axis=0)\n",
    "    \n",
    "        \n",
    "    return initial_centroids,initial_clusters   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zTeS_3skpbaU"
   },
   "source": [
    "KMeans Algorithm with K = 2, that gives optimal centroids of the two clusters found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z90EYezrD9dz"
   },
   "outputs": [],
   "source": [
    "def KMeans_clustering(reduced_matrix,K):\n",
    "    \n",
    "    rows = np.shape(reduced_matrix)[0]\n",
    "    initial_clusters = np.mat(np.zeros((rows,2)))\n",
    "    initial_centroids = initialize_centroids(reduced_matrix,K)\n",
    "    \n",
    "    initial_centroids,initial_clusters = calculate_optimalcentroids(reduced_matrix,initial_centroids,initial_clusters,K,rows)\n",
    "    \n",
    "    \n",
    "    return initial_centroids,initial_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGY9KIOsD_0h"
   },
   "outputs": [],
   "source": [
    "def find_clusters(reduced_matrix,K,iterations,centroids,clusters,minimum_SSE):\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        initial_centroids,initial_clusters = KMeans_clustering(reduced_matrix,K)\n",
    "        \n",
    "        cal_SSE = np.sum(initial_clusters,axis=0)[0,1]\n",
    "        \n",
    "        if cal_SSE < minimum_SSE:\n",
    "            minimum_SSE = cal_SSE\n",
    "\n",
    "            store_centroids = initial_centroids\n",
    "            store_clusters = initial_clusters\n",
    "    \n",
    "    centroids.append(store_centroids[0].tolist()[0])\n",
    "    centroids.append(store_centroids[1].tolist()[0])\n",
    "    clusters = store_clusters\n",
    "    \n",
    "    return centroids,clusters,minimum_SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoxKlKgMEByE"
   },
   "outputs": [],
   "source": [
    "def select_cluster_withmaxSSE(centroids,clusters,maximum_SSE,index):\n",
    "    \n",
    "    for c in range(len(centroids)):\n",
    "        cal_SSE = np.sum(clusters[np.nonzero(clusters[:,0] == c)[0]])\n",
    "        if cal_SSE > maximum_SSE:\n",
    "            index = c\n",
    "            maximum_SSE = cal_SSE\n",
    "            \n",
    "    return index,maximum_SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HblQBcQTEDek"
   },
   "outputs": [],
   "source": [
    "def store_cluster_withminSSE(reduced_matrix,clusters,minimum_SSE,K,iterations,index):\n",
    "    \n",
    "    for i in range(iterations):\n",
    "            store_clusters = reduced_matrix[np.nonzero(clusters[:,0] == index)[0]]\n",
    "            initial_centroids,initial_clusters = KMeans_clustering(store_clusters, 2)\n",
    "            cal_SSE = np.sum(clusters[:,1],axis=0)\n",
    "            \n",
    "            if cal_SSE < minimum_SSE:\n",
    "                minimum_SSE = cal_SSE\n",
    "                temp_centroids = initial_centroids.copy()\n",
    "                temp_clusters = initial_clusters.copy()\n",
    "                \n",
    "    return temp_centroids,temp_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSYNsxH7EFOb"
   },
   "outputs": [],
   "source": [
    "def update_cluster_details(temp_centroids,temp_clusters,index,centroids,clusters):\n",
    "    \n",
    "    temp_clusters[np.nonzero(temp_clusters[:,0]==1)[0],0] = len(centroids)\n",
    "    temp_clusters[np.nonzero(temp_clusters[:,0]==0)[0],0] = index\n",
    "\n",
    "    clusters[np.nonzero(clusters[:,0] == index)[0],:] = temp_clusters\n",
    "    centroids[index]=temp_centroids[0].tolist()[0]\n",
    "    centroids.append(temp_centroids[1].tolist()[0])\n",
    "    \n",
    "    return centroids,clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fq1V59xTplVk"
   },
   "source": [
    "Find two optimal clusters and run the K means algorithm till we reach K  =7 where we always bisect the larger SSE cluster and append all smaller SSE clusters to our foinal list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_uLP8LBEHRP"
   },
   "outputs": [],
   "source": [
    "def Bisecting_KMeans(reduced_matrix,K,iterations):\n",
    "    \n",
    "    rows = reduced_matrix.shape[0]\n",
    "    columns = reduced_matrix.shape[1]\n",
    "    centroids = []\n",
    "    clusters = np.mat(np.zeros((rows,2)))\n",
    "    minimum_SSE = np.inf\n",
    "    \n",
    "    centroids,clusters,minimum_SSE = find_clusters(reduced_matrix,2,iterations,centroids,clusters,minimum_SSE)\n",
    "    \n",
    "    minimum_SSE = np.inf\n",
    "    \n",
    "    while len(centroids) < K :\n",
    "        index = -1\n",
    "        maximum_SSE = -1\n",
    "        \n",
    "        index,maximum_SSE = select_cluster_withmaxSSE(centroids,clusters,maximum_SSE,index)\n",
    "        temp_centroids,temp_clusters = store_cluster_withminSSE(reduced_matrix,clusters,minimum_SSE,K,iterations,index)\n",
    "        centroids,clusters = update_cluster_details(temp_centroids,temp_clusters,index,centroids,clusters)\n",
    " \n",
    "    return centroids,clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "toO6uE-rp21U"
   },
   "source": [
    "At K = 7 Save the document ID and cluster ID as format.dat file to validate the NMI score in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8e4mN-LefnSN"
   },
   "outputs": [],
   "source": [
    "def save_clusterinfo(reduced_matrix,list_of_clusters):\n",
    "    doc_id = []\n",
    "    for i in range(1,reduced_matrix.shape[0]+1):\n",
    "        doc_id.append(i)\n",
    "\n",
    "    output = open(\"format_final2.dat\", \"w\")\n",
    "    print(\"{0}{1}{2}\".format('ItemID',',','ClusterID'),file=output)\n",
    "    for document_id,value in zip(doc_id,list_of_clusters[:, 0]+1):\n",
    "        print(\"{0}{1}{2}\".format(document_id,',',int(value.A[0][0])), file=output)\n",
    "\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLYRlY_nZZho"
   },
   "outputs": [],
   "source": [
    "def run_bi_kmeans_plot_metrics(reduced_matrix,K,iterations):\n",
    "\n",
    "    scores = []\n",
    "    K_values = []\n",
    "    cluster_labels = []\n",
    "    \n",
    "    for K in range(3,22,2):\n",
    "\n",
    "        list_of_centroids,list_of_clusters = Bisecting_KMeans(reduced_matrix, K, iterations)\n",
    "\n",
    "        if K == 7:\n",
    "            save_clusterinfo(reduced_matrix,list_of_clusters) \n",
    "\n",
    "        for value in list_of_clusters[:, 0]+1:\n",
    "            cluster_labels.append((int(value.A[0][0])))\n",
    "        \n",
    "        K_values.append(K)\n",
    "        scores.append(metrics.silhouette_score(reduced_matrix, cluster_labels))\n",
    "        cluster_labels = []\n",
    "\n",
    "    plt.plot(K_values, scores)\n",
    "    plt.xticks(scores, scores)\n",
    "    plt.xlabel('Number of Clusters k')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.grid(linestyle='dotted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "bHkvl4c_Ztsz",
    "outputId": "f222a2d3-a6c7-47a7-de22-19cd770e15ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8580, 126356)\n",
      "tfidf done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:1466: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  idf = np.log(n_samples / df) + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after reducing the features (8580, 150)\n",
      "done bisec\n",
      "done bisec\n",
      "done bisec\n",
      "done bisec\n"
     ]
    }
   ],
   "source": [
    "matrix = convert_dat_csrmatrix('train.dat')\n",
    "print(matrix.shape)\n",
    "\n",
    "matrix_tfid = calculate_tfidf(matrix)\n",
    "print(\"tfidf done\")\n",
    "\n",
    "reduced_matrix = reduce_dimensionality(matrix_tfid)\n",
    "print(\"Dataset after reducing the features\", reduced_matrix.shape)\n",
    "\n",
    "run_bi_kmeans_plot_metrics(reduced_matrix,7,20)\n",
    "print(list_of_clusters)\n",
    "len(list_of_clusters)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bisecting K Means text clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
